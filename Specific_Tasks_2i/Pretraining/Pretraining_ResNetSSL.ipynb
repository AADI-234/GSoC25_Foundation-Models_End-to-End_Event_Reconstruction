{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13640218",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-02T14:27:57.393017Z",
     "iopub.status.busy": "2025-04-02T14:27:57.392714Z",
     "iopub.status.idle": "2025-04-02T14:28:05.845954Z",
     "shell.execute_reply": "2025-04-02T14:28:05.845248Z"
    },
    "papermill": {
     "duration": 8.459723,
     "end_time": "2025-04-02T14:28:05.847508",
     "exception": false,
     "start_time": "2025-04-02T14:27:57.387785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import GaussianBlur\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, mean_squared_error, f1_score, confusion_matrix, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from scipy.stats import gaussian_kde\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Device configuration and seeding\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec9800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.867510Z",
     "iopub.status.busy": "2025-04-02T14:28:05.867288Z",
     "iopub.status.idle": "2025-04-02T14:28:05.873188Z",
     "shell.execute_reply": "2025-04-02T14:28:05.872589Z"
    },
    "papermill": {
     "duration": 0.010704,
     "end_time": "2025-04-02T14:28:05.874406",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.863702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Augmentation:\n",
    "    def __init__(self, size=112, crop_scale=(0.8, 1.0), brightness=(0.8, 1.2)):\n",
    "        self.size = size\n",
    "        self.crop_scale = crop_scale\n",
    "        self.brightness = brightness\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x is a torch tensor image of shape (C, H, W)\n",
    "        _, h, w = x.shape\n",
    "        \n",
    "        # Random Resized Crop\n",
    "        scale = random.uniform(self.crop_scale[0], self.crop_scale[1])\n",
    "        new_h = int(h * scale)\n",
    "        new_w = int(w * scale)\n",
    "        if h > new_h and w > new_w:\n",
    "            top = random.randint(0, h - new_h)\n",
    "            left = random.randint(0, w - new_w)\n",
    "            x = x[:, top:top+new_h, left:left+new_w]\n",
    "        # Resize to the target size\n",
    "        x = F.interpolate(x.unsqueeze(0), size=(self.size, self.size),\n",
    "                          mode='bilinear', align_corners=False).squeeze(0)\n",
    "        \n",
    "        # Random Horizontal Flip\n",
    "        if random.random() < 0.5:\n",
    "            x = torch.flip(x, dims=[2])\n",
    "            \n",
    "        # Random brightness jitter\n",
    "        if random.random() < 0.5:\n",
    "            factor = random.uniform(self.brightness[0], self.brightness[1])\n",
    "            x = x * factor\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1df507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.881047Z",
     "iopub.status.busy": "2025-04-02T14:28:05.880839Z",
     "iopub.status.idle": "2025-04-02T14:28:05.885473Z",
     "shell.execute_reply": "2025-04-02T14:28:05.884899Z"
    },
    "papermill": {
     "duration": 0.009228,
     "end_time": "2025-04-02T14:28:05.886640",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.877412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class JetDatasetSSL(Dataset):\n",
    "    def __init__(self, file_path, key, transform=None):\n",
    "        self.file_path = file_path\n",
    "        self.key = key\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            self.length = f[key].shape[0]\n",
    "        self.transform = transforms.Compose([Augmentation()]) if transform is None else transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as f:\n",
    "            jet = f[self.key][idx]  # Expected shape: [125, 125, 8]\n",
    "        # Convert shape to [channels, eta, phi]\n",
    "        jet = torch.tensor(jet, dtype=torch.float32).permute(2, 0, 1)\n",
    "        # Create two augmented views for contrastive learning\n",
    "        view1 = self.transform(jet)\n",
    "        view2 = self.transform(jet)\n",
    "        return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b472af6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.893428Z",
     "iopub.status.busy": "2025-04-02T14:28:05.893181Z",
     "iopub.status.idle": "2025-04-02T14:28:05.898726Z",
     "shell.execute_reply": "2025-04-02T14:28:05.898082Z"
    },
    "papermill": {
     "duration": 0.010264,
     "end_time": "2025-04-02T14:28:05.899908",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.889644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New class for labeled dataset\n",
    "class JetDatasetLabeled(Dataset):\n",
    "    def __init__(self, file_path, jet_key=\"jet\", y_key=\"Y\", pt_key=\"pT\", m_key=\"m\"):\n",
    "        self.file_path = file_path\n",
    "        self.jet_key = jet_key\n",
    "        self.y_key = y_key\n",
    "        self.pt_key = pt_key\n",
    "        self.m_key = m_key\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            self.length = f[jet_key].shape[0]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as f:\n",
    "            jet = f[self.jet_key][idx]\n",
    "            y = f[self.y_key][idx]\n",
    "            pt = f[self.pt_key][idx]\n",
    "            m = f[self.m_key][idx]\n",
    "            \n",
    "        # Convert shape to [channels, eta, phi]\n",
    "        jet = torch.tensor(jet, dtype=torch.float32).permute(2, 0, 1)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        pt = torch.tensor(pt, dtype=torch.float32)\n",
    "        m = torch.tensor(m, dtype=torch.float32)\n",
    "        \n",
    "        return jet, y, pt, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace73a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.906360Z",
     "iopub.status.busy": "2025-04-02T14:28:05.906134Z",
     "iopub.status.idle": "2025-04-02T14:28:05.911746Z",
     "shell.execute_reply": "2025-04-02T14:28:05.911113Z"
    },
    "papermill": {
     "duration": 0.010268,
     "end_time": "2025-04-02T14:28:05.913071",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.902803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNetSSL(nn.Module):\n",
    "    def __init__(self, latent_dim=256, projection_dim=128, resnet_depth=18):\n",
    "        super().__init__()\n",
    "\n",
    "        if resnet_depth == 18:\n",
    "            self.encoder_backbone = models.resnet18(weights=None)\n",
    "            num_bottleneck_features = 512\n",
    "        elif resnet_depth == 34:\n",
    "            self.encoder_backbone = models.resnet34(weights=None)\n",
    "            num_bottleneck_features = 512\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet depth\")\n",
    "\n",
    "        original_conv1 = self.encoder_backbone.conv1\n",
    "        self.encoder_backbone.conv1 = nn.Conv2d(\n",
    "            8,\n",
    "            original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.encoder_backbone.fc = nn.Identity()\n",
    "\n",
    "        self.final_layer = nn.Linear(num_bottleneck_features, latent_dim)\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim * 2, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features_bottleneck = self.encoder_backbone(x)\n",
    "        features_latent = self.final_layer(features_bottleneck)\n",
    "        projections = self.projector(features_latent)\n",
    "        return features_latent, projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c55696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.925377Z",
     "iopub.status.busy": "2025-04-02T14:28:05.925104Z",
     "iopub.status.idle": "2025-04-02T14:28:05.930497Z",
     "shell.execute_reply": "2025-04-02T14:28:05.929686Z"
    },
    "papermill": {
     "duration": 0.009979,
     "end_time": "2025-04-02T14:28:05.931701",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.921722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LARS(optim.Optimizer):\n",
    "    def __init__(self, optimizer, eps=1e-9, trust_coef=0.002):\n",
    "        \"\"\"\n",
    "        LARS wrapper for an optimizer with updated hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            optimizer: The inner optimizer (e.g. AdamW).\n",
    "            eps: Small value for numerical stability.\n",
    "            trust_coef: Coefficient for computing the local learning rate.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.eps = eps\n",
    "        self.trust_coef = trust_coef\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            weight_decay = group.get(\"weight_decay\", 0)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p.data, alpha=weight_decay)\n",
    "                w_norm = p.data.norm()\n",
    "                g_norm = d_p.norm()\n",
    "                if w_norm > 0 and g_norm > 0:\n",
    "                    local_lr = self.trust_coef * w_norm / (g_norm + self.eps)\n",
    "                    d_p.mul_(local_lr)\n",
    "        self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a3c700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.938269Z",
     "iopub.status.busy": "2025-04-02T14:28:05.938010Z",
     "iopub.status.idle": "2025-04-02T14:28:05.943957Z",
     "shell.execute_reply": "2025-04-02T14:28:05.943160Z"
    },
    "papermill": {
     "duration": 0.010545,
     "end_time": "2025-04-02T14:28:05.945159",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.934614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05, eps=1e-6):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "\n",
    "        # Normalize the input embeddings\n",
    "        z_i = F.normalize(z_i, dim=1, eps=self.eps)\n",
    "        z_j = F.normalize(z_j, dim=1, eps=self.eps)\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "        # Compute similarity matrix (use float32 for stability)\n",
    "        similarity_matrix = torch.mm(representations, representations.t()) / self.temperature\n",
    "        similarity_matrix = similarity_matrix.float()\n",
    "\n",
    "        # Mask out self-similarity using a safer mask value\n",
    "        mask = torch.eye(2 * batch_size, device=z_i.device).bool()\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, -1e4)\n",
    "\n",
    "        # Compute positive and negative samples\n",
    "        positives = torch.cat([\n",
    "            torch.diag(similarity_matrix, batch_size),\n",
    "            torch.diag(similarity_matrix, -batch_size)\n",
    "        ])\n",
    "\n",
    "        negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "\n",
    "        # Calculate the loss using logsumexp for numerical stability\n",
    "        logsumexp_negatives = torch.logsumexp(negatives, dim=1)\n",
    "        loss = -torch.log(torch.exp(positives) / (torch.exp(logsumexp_negatives) + self.eps)).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77261d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.952064Z",
     "iopub.status.busy": "2025-04-02T14:28:05.951861Z",
     "iopub.status.idle": "2025-04-02T14:28:05.964525Z",
     "shell.execute_reply": "2025-04-02T14:28:05.963887Z"
    },
    "papermill": {
     "duration": 0.017627,
     "end_time": "2025-04-02T14:28:05.965817",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.948190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ssl(model, train_loader, num_epochs=100, save_dir=\"./models\", validation_loader=None, patience=5, use_lars=True):\n",
    "    \"\"\"\n",
    "    Train the SSL model with improved monitoring, validation, early stopping, and optional LARS.\n",
    "    \n",
    "    Args:\n",
    "        model: The SimCLR model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        num_epochs: Number of training epochs.\n",
    "        save_dir: Directory to save model checkpoints.\n",
    "        validation_loader: Optional loader for validation.\n",
    "        patience: Number of epochs with no improvement after which training will be stopped.\n",
    "        use_lars: If True, wraps the underlying optimizer with LARS.\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and training history.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup base optimizer using AdamW\n",
    "    base_optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=4e-5)\n",
    "    optimizer = LARS(base_optimizer, eps=1e-9, trust_coef=0.002) if use_lars else base_optimizer\n",
    "    \n",
    "    # Use the inner optimizer for the scheduler if LARS wrapper is used\n",
    "    scheduler = CosineAnnealingLR(optimizer.optimizer if use_lars else optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Contrastive loss with temperature scaling\n",
    "    criterion = NTXentLoss(temperature=0.05)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    early_stop_counter = 0\n",
    "    scaler = GradScaler(device=device) if device.type == \"cuda\" else GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for view1, view2 in pbar:\n",
    "            x1, x2 = view1.to(device), view2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type=device.type):\n",
    "                _, proj1 = model(x1)\n",
    "                _, proj2 = model(x2)\n",
    "                loss = criterion(proj1, proj2)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer.optimizer if use_lars else optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            current_lr = (optimizer.optimizer.param_groups[0]['lr'] if use_lars \n",
    "                          else optimizer.param_groups[0]['lr'])\n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': current_lr})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        current_loss = avg_loss\n",
    "        if validation_loader is not None:\n",
    "            model.eval()\n",
    "            val_total_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for view1, view2 in validation_loader:\n",
    "                    x1, x2 = view1.to(device), view2.to(device)\n",
    "                    _, proj1 = model(x1)\n",
    "                    _, proj2 = model(x2)\n",
    "                    loss = criterion(proj1, proj2)\n",
    "                    val_total_loss += loss.item()\n",
    "            val_loss = val_total_loss / len(validation_loader)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            print(f\"Epoch {epoch+1} - Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            current_loss = val_loss\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} - Train Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': (optimizer.optimizer.state_dict() if use_lars \n",
    "                                         else optimizer.state_dict()),\n",
    "                'loss': best_loss,\n",
    "                'history': history,\n",
    "            }, os.path.join(save_dir, \"best_model.pt\"))\n",
    "            print(f\"New best model saved! Loss: {best_loss:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1} with best loss {best_loss:.4f}\")\n",
    "                break\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': (optimizer.optimizer.state_dict() if use_lars \n",
    "                                     else optimizer.state_dict()),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': current_loss,\n",
    "            'history': history,\n",
    "        }, os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "            \n",
    "        scheduler.step()\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    if history['val_loss']:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.semilogy(history['train_loss'], label='Training Loss')\n",
    "    if history['val_loss']:\n",
    "        plt.semilogy(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.legend()\n",
    "    plt.title('Loss on Log Scale')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_curve.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e284f803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.972791Z",
     "iopub.status.busy": "2025-04-02T14:28:05.972547Z",
     "iopub.status.idle": "2025-04-02T14:28:05.977349Z",
     "shell.execute_reply": "2025-04-02T14:28:05.976544Z"
    },
    "papermill": {
     "duration": 0.009499,
     "end_time": "2025-04-02T14:28:05.978541",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.969042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract features using the pre-trained encoder\n",
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    y_list = []\n",
    "    pt_list = []\n",
    "    m_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for jets, y, pt, m in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            jets = jets.to(device)\n",
    "            features, _ = model(jets)\n",
    "            features_list.append(features.cpu())\n",
    "            y_list.append(y)\n",
    "            pt_list.append(pt)\n",
    "            m_list.append(m)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    features = torch.cat(features_list, dim=0)\n",
    "    y = torch.cat(y_list, dim=0)\n",
    "    pt = torch.cat(pt_list, dim=0)\n",
    "    m = torch.cat(m_list, dim=0)\n",
    "    \n",
    "    return features, y, pt, m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbaddb5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T14:28:05.985373Z",
     "iopub.status.busy": "2025-04-02T14:28:05.985109Z",
     "iopub.status.idle": "2025-04-02T16:22:08.082313Z",
     "shell.execute_reply": "2025-04-02T16:22:08.081314Z"
    },
    "papermill": {
     "duration": 6842.858196,
     "end_time": "2025-04-02T16:22:08.839782",
     "exception": false,
     "start_time": "2025-04-02T14:28:05.981586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SSL dataset...\n",
      "Dataset loaded with 60000 samples\n",
      "Initializing SimCLR model on cuda\n",
      "Model created with 11,520,768 parameters\n",
      "Starting SSL training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 421/421 [03:39<00:00,  1.92it/s, loss=2.11, lr=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 3.6512, Val Loss: 2.2338\n",
      "New best model saved! Loss: 2.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 421/421 [03:41<00:00,  1.90it/s, loss=0.795, lr=0.000299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 1.3974, Val Loss: 0.7458\n",
      "New best model saved! Loss: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 421/421 [03:39<00:00,  1.92it/s, loss=0.285, lr=0.000297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.4619, Val Loss: 0.2982\n",
      "New best model saved! Loss: 0.2982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 421/421 [03:37<00:00,  1.94it/s, loss=0.0984, lr=0.000293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.1986, Val Loss: 0.2346\n",
      "New best model saved! Loss: 0.2346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 421/421 [03:35<00:00,  1.95it/s, loss=0.0704, lr=0.000287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.0952, Val Loss: 0.0921\n",
      "New best model saved! Loss: 0.0921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 421/421 [03:32<00:00,  1.98it/s, loss=0.0276, lr=0.00028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.0478, Val Loss: 0.0393\n",
      "New best model saved! Loss: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 421/421 [03:42<00:00,  1.90it/s, loss=0.034, lr=0.000271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.0379, Val Loss: 0.0322\n",
      "New best model saved! Loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 421/421 [03:40<00:00,  1.91it/s, loss=0.0285, lr=0.000261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.0314, Val Loss: 0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 421/421 [03:34<00:00,  1.96it/s, loss=0.0154, lr=0.00025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.0269, Val Loss: 0.0249\n",
      "New best model saved! Loss: 0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 421/421 [03:34<00:00,  1.96it/s, loss=0.0215, lr=0.000238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.0194, Val Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 421/421 [03:35<00:00,  1.95it/s, loss=0.00752, lr=0.000225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train Loss: 0.0139, Val Loss: 0.0118\n",
      "New best model saved! Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 421/421 [03:50<00:00,  1.83it/s, loss=0.016, lr=0.000211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Loss: 0.0122, Val Loss: 0.0114\n",
      "New best model saved! Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 421/421 [03:51<00:00,  1.82it/s, loss=0.00743, lr=0.000196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Loss: 0.0104, Val Loss: 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 421/421 [03:47<00:00,  1.85it/s, loss=0.00885, lr=0.000181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train Loss: 0.0095, Val Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 421/421 [03:42<00:00,  1.90it/s, loss=0.00967, lr=0.000166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train Loss: 0.0077, Val Loss: 0.0068\n",
      "New best model saved! Loss: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 421/421 [03:48<00:00,  1.84it/s, loss=0.00265, lr=0.00015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train Loss: 0.0065, Val Loss: 0.0067\n",
      "New best model saved! Loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 421/421 [03:36<00:00,  1.95it/s, loss=0.00529, lr=0.000134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train Loss: 0.0061, Val Loss: 0.0052\n",
      "New best model saved! Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 421/421 [03:38<00:00,  1.93it/s, loss=0.00795, lr=0.000119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train Loss: 0.0057, Val Loss: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 421/421 [03:41<00:00,  1.90it/s, loss=0.00473, lr=0.000104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train Loss: 0.0055, Val Loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 421/421 [03:40<00:00,  1.91it/s, loss=0.00289, lr=8.9e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Loss: 0.0052, Val Loss: 0.0044\n",
      "New best model saved! Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 421/421 [03:38<00:00,  1.93it/s, loss=0.00192, lr=7.5e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Train Loss: 0.0050, Val Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 421/421 [03:35<00:00,  1.95it/s, loss=0.00365, lr=6.18e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Train Loss: 0.0048, Val Loss: 0.0043\n",
      "New best model saved! Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 421/421 [03:34<00:00,  1.96it/s, loss=0.00371, lr=4.96e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Train Loss: 0.0044, Val Loss: 0.0037\n",
      "New best model saved! Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 421/421 [03:37<00:00,  1.94it/s, loss=0.00612, lr=3.85e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Train Loss: 0.0039, Val Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 421/421 [03:35<00:00,  1.95it/s, loss=0.0077, lr=2.86e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Train Loss: 0.0044, Val Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 421/421 [03:35<00:00,  1.95it/s, loss=0.00213, lr=2.01e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Train Loss: 0.0040, Val Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 421/421 [03:37<00:00,  1.94it/s, loss=0.0162, lr=1.3e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Train Loss: 0.0041, Val Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 421/421 [03:35<00:00,  1.95it/s, loss=0.00334, lr=7.34e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Train Loss: 0.0040, Val Loss: 0.0039\n",
      "Early stopping triggered at epoch 28 with best loss 0.0037\n",
      "SSL training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    ssl_file_path = \"/kaggle/input/dataset-specific-unlabelled/Dataset_Specific_Unlabelled.h5\"\n",
    "    labeled_file_path = \"/kaggle/input/dataset-specific-labelled-full-only-for-2i/Dataset_Specific_labelled_full_only_for_2i.h5\"\n",
    "    batch_size = 128\n",
    "    num_epochs_ssl = 30  # Increased for better convergence\n",
    "    latent_dim = 256\n",
    "    save_dir = \"./models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Step 1: Self-Supervised Learning Pretraining\n",
    "    print(\"Loading SSL dataset...\")\n",
    "    try:\n",
    "        ssl_dataset = JetDatasetSSL(ssl_file_path, \"jet\")\n",
    "        print(f\"Dataset loaded with {len(ssl_dataset)} samples\")\n",
    "        \n",
    "        # Create train/validation split for SSL\n",
    "        train_size = int(0.9 * len(ssl_dataset))\n",
    "        val_size = len(ssl_dataset) - train_size\n",
    "        ssl_train_dataset, ssl_val_dataset = torch.utils.data.random_split(\n",
    "            ssl_dataset, [train_size, val_size], \n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        ssl_train_loader = DataLoader(\n",
    "            ssl_train_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        ssl_val_loader = DataLoader(\n",
    "            ssl_val_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        print(f\"Initializing SimCLR model on {device}\")\n",
    "        ssl_model = model = ResNetSSL(latent_dim=256, projection_dim=128, resnet_depth=18).to(device)\n",
    "        \n",
    "        # Print model summary\n",
    "        total_params = sum(p.numel() for p in ssl_model.parameters())\n",
    "        print(f\"Model created with {total_params:,} parameters\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting SSL training...\")\n",
    "        ssl_model, history = train_ssl(\n",
    "            ssl_model, \n",
    "            ssl_train_loader, \n",
    "            num_epochs=num_epochs_ssl, \n",
    "            save_dir=save_dir,\n",
    "            validation_loader=ssl_val_loader ,use_lars=True)  \n",
    "        \n",
    "        print(\"SSL training completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during SSL training: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6809564,
     "sourceId": 10947806,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6898490,
     "sourceId": 11070195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6862.311693,
   "end_time": "2025-04-02T16:22:17.104023",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-02T14:27:54.792330",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
